import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional

# å°è¯•å¯¼å…¥ç¯å¢ƒ
from bfcl_env import build_bfclv4_envs
from openai import OpenAI
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. BFCL ç¯å¢ƒé…ç½®
BFCL_MODEL_KEY = "Qwen/Qwen3-8B"
TEST_CATEGORY = "live"
ENV_NUM = 15  # æ¯ä¸ª batch å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆä¸ bfcl_env_test ä¿æŒä¸€è‡´è¯­ä¹‰ï¼‰
GROUP_N = 1
SEED = 42
MAX_SAMPLES: Optional[int] = None  # æœ€å¤šè¯„ä¼°å¤šå°‘ä¸ªæ ·æœ¬ï¼ŒNone è¡¨ç¤ºè·‘å®Œæ•´ä¸ªæ•°æ®é›†

# 2. æœ¬åœ° vLLM é…ç½®
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
VLLM_API_URL = "http://127.0.0.1:8000/v1"
VLLM_MODEL_NAME = "Qwen/Qwen3-8B"
MODEL_PATH = "Qwen/Qwen3-8B"

STOP_TOKENS = ["<|im_end|>", "<|endoftext|>", "</s>"]

# å¤šçº¿ç¨‹é…ç½®ï¼ˆä»ç„¶æ˜¯â€œå•æ¡ prompt è°ƒä¸€æ¬¡ APIâ€ï¼Œåªæ˜¯å¹¶è¡Œå¾ˆå¤šæ¡ï¼‰
MAX_WORKERS = 100  # æœ€å¤§çº¿ç¨‹æ•°
# ===========================================


def get_vllm_client():
    return OpenAI(base_url=VLLM_API_URL, api_key="EMPTY")


def generate_single_action(
    client: OpenAI,
    prompt: str,
    index: int,
) -> tuple:
    """
    å•ä¸ªè¯·æ±‚å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹è°ƒç”¨ï¼ˆä¿æŒâ€œå•æ¡è°ƒç”¨ APIâ€çš„å½¢å¼ï¼‰

    Args:
        client: OpenAI å®¢æˆ·ç«¯
        prompt: å­—ç¬¦ä¸²æ ¼å¼çš„æç¤ºè¯
        index: åŸå§‹ç´¢å¼•

    Returns:
        (index, action) å…ƒç»„
    """
    try:
        response = client.completions.create(
            model=VLLM_MODEL_NAME,
            prompt=prompt,  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸² prompt
            max_tokens=4096,
            temperature=1,
            stop=STOP_TOKENS,
        )
        action = response.choices[0].text.strip()
        return (index, action)
    except Exception as e:
        tqdm.write(f"âš ï¸ è¯·æ±‚å¤±è´¥ (index {index}): {e}")
        return (index, "")


def generate_actions_openai_batch(
    client: OpenAI,
    observations: List[str],
    dones: List[bool],
) -> List[str]:
    """
    ä½¿ç”¨å¤šçº¿ç¨‹æ–¹å¼å¹¶å‘è¯·æ±‚ vLLM API
    æ¯ä¸ªè¯·æ±‚å‘é€å­—ç¬¦ä¸²æ ¼å¼çš„ promptï¼ˆè€Œé token IDsï¼‰
    æ³¨æ„ï¼šè¿™é‡Œçš„â€œbatchâ€åªæ˜¯çº¿ç¨‹å¹¶å‘ï¼Œä»ç„¶æ˜¯å¯¹æ¯æ¡ prompt å•ç‹¬è°ƒç”¨ä¸€æ¬¡ APIã€‚

    Args:
        client: OpenAI å®¢æˆ·ç«¯
        observations: è§‚å¯Ÿåˆ—è¡¨
        dones: å®ŒæˆçŠ¶æ€åˆ—è¡¨

    Returns:
        åŠ¨ä½œåˆ—è¡¨
    """
    total = len(observations)
    actions = [None] * total

    # æ”¶é›†éœ€è¦å¤„ç†çš„è¯·æ±‚ä»»åŠ¡
    tasks = []
    for i, (obs, done) in enumerate(zip(observations, dones)):
        if done:
            actions[i] = "FINISHED"
        elif not obs:
            actions[i] = ""
        else:
            tasks.append((i, obs))

    if not tasks:
        # å¡«å……å‰©ä½™çš„ None ä¸ºç©ºå­—ç¬¦ä¸²
        for i in range(total):
            if actions[i] is None:
                actions[i] = ""
        return actions

    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè¯·æ±‚
    max_workers = min(len(tasks), MAX_WORKERS)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_index = {executor.submit(generate_single_action, client, obs, idx): idx for idx, obs in tasks}

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_index):
            try:
                idx, action = future.result()
                actions[idx] = action
            except Exception as e:
                original_idx = future_to_index[future]
                tqdm.write(f"âš ï¸ çº¿ç¨‹æ‰§è¡Œå¤±è´¥ (index {original_idx}): {e}")
                actions[original_idx] = ""

    # ç¡®ä¿æ‰€æœ‰ä½ç½®éƒ½æœ‰å€¼
    for i in range(total):
        if actions[i] is None:
            actions[i] = ""

    return actions


def main():
    print(f"ğŸš€ åˆå§‹åŒ– BFCL è¯„æµ‹ | Model: {BFCL_MODEL_KEY} | Batch EnvNum: {ENV_NUM}")

    client = get_vllm_client()

    try:
        # è¿™é‡Œçš„ env_num è¯­ä¹‰ä¸ bfcl_env_test.py ä¿æŒä¸€è‡´ï¼šå•æ¬¡ reset äº§ç”Ÿä¸€ä¸ª batch
        envs = build_bfclv4_envs(
            env_name=f"bfcl-{TEST_CATEGORY}",
            seed=SEED,
            env_num=ENV_NUM,
            group_n=GROUP_N,
            resources_per_worker={},
            model_name=BFCL_MODEL_KEY,
            is_train=False,
        )
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæ„å»ºå¤±è´¥: {e}")
        return

    # è·å–æ•°æ®é›†æ€»å¤§å°ï¼ˆä¸ bfcl_env_test.py å¯¹é½ï¼‰
    total_dataset_len = len(envs.prompt_entries_total) if hasattr(envs, "prompt_entries_total") else None
    if total_dataset_len is None:
        print("âš ï¸ æ— æ³•è·å–æ•°æ®é›†æ€»å¤§å°ï¼Œå°†ä½¿ç”¨å½“å‰ batch å¤§å°")
        total_dataset_len = ENV_NUM if ENV_NUM > 0 else 1

    print(f"ğŸ“Š Dataset Size: {total_dataset_len} | Batch Size: {ENV_NUM} | Group N: {GROUP_N}")

    # ===== å…¨å±€ç»Ÿè®¡ï¼ˆè·¨ batchï¼‰=====
    global_processed_count = 0
    global_final_rewards = []
    group_stats = [[] for _ in range(GROUP_N)]

    pbar_total = min(total_dataset_len, MAX_SAMPLES) if MAX_SAMPLES is not None else total_dataset_len
    pbar = tqdm(total=pbar_total, desc="Total Progress", unit="sample", ncols=100)

    max_steps = 300

    # ================= å¤–å±‚å¾ªç¯ï¼šå¤„ç†å¤šä¸ª batch =================
    while True:
        if MAX_SAMPLES is not None and global_processed_count >= MAX_SAMPLES:
            print(f"\nâœ… å·²è¾¾åˆ°æŒ‡å®šæ ·æœ¬æ•°é‡ {MAX_SAMPLES}ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break

        try:
            print("ğŸ”„ Environment Reset...")
            observations, infos = envs.reset()
        except StopIteration:
            print("\nâœ… å·²å¤„ç†å®Œæ‰€æœ‰æ•°æ®ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break
        except Exception as e:
            print(f"\nâš ï¸ Environment Reset å‡ºé”™: {e}ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break

        current_batch_size = len(observations)

        if current_batch_size == 0:
            print("\nâœ… æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break

        # å¦‚æœå½“å‰ batch ä¼šè¶…è¿‡ MAX_SAMPLESï¼Œåˆ™åªå¤„ç†éƒ¨åˆ†æ ·æœ¬
        if MAX_SAMPLES is not None:
            remaining = MAX_SAMPLES - global_processed_count
            if remaining <= 0:
                break
            if current_batch_size > remaining:
                observations = observations[:remaining]
                current_batch_size = remaining

        # è®¡ç®—å½“å‰ batch æ¯ä¸ªä»»åŠ¡çš„ Group IDï¼ˆä¸ bfcl_env_test.py å¯¹é½ï¼‰
        batch_group_ids = []
        for i in range(current_batch_size):
            abs_index = global_processed_count + i
            group_id = abs_index % GROUP_N
            batch_group_ids.append(group_id)

        dones = [False] * current_batch_size
        episode_rewards = [0.0] * current_batch_size
        final_rewards = [None] * current_batch_size

        step_cnt = 0

        print(f"ğŸ“Š å½“å‰ batch å¤§å°: {current_batch_size}ï¼Œæœ€å¤§æ­¥æ•° {max_steps}...")

        # ================= å†…å±‚å¾ªç¯ï¼šå¤„ç†å•ä¸ª batch çš„ episode =================
        while not all(dones) and step_cnt < max_steps:
            step_cnt += 1

            # è®¡ç®—å½“å‰æœªå®Œæˆçš„æ•°é‡ï¼ˆåŸºäºæŒ‰ instance ä¸‹æ ‡çš„ final_rewardsï¼‰
            n_done_so_far = sum(1 for r in final_rewards if r is not None)
            active_count = current_batch_size - n_done_so_far
            tqdm.write(f"Batch Step {step_cnt}/{max_steps} | Active Envs: {active_count} | Generating Actions...")

            # === ç”ŸæˆåŠ¨ä½œï¼ˆä¿æŒâ€œå•æ¡è°ƒç”¨ APIâ€çš„å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰ ===
            actions = generate_actions_openai_batch(
                client,
                observations,
                dones,
            )

            # === ç¯å¢ƒæ­¥è¿› ===
            next_obs, step_rewards, next_dones, step_infos = envs.step(actions)

            # ç»Ÿè®¡æœ¬è½®æœ‰å¤šå°‘ä¸ªå˜æˆ Done
            finished_in_this_step = 0

            # === æ•°æ®æ›´æ–° ===
            for i in range(current_batch_size):
                if not dones[i]:
                    episode_rewards[i] += step_rewards[i]

                    if next_dones[i]:
                        final_rewards[i] = episode_rewards[i]
                        finished_in_this_step += 1

                    observations[i] = next_obs[i]
                    dones[i] = next_dones[i]

        # ================= æ”¶é›†å½“å‰ batch çš„ç»“æœ =================
        for i in range(current_batch_size):
            reward = final_rewards[i] if final_rewards[i] is not None else 0.0
            global_final_rewards.append(reward)

            g_id = batch_group_ids[i]
            group_stats[g_id].append(reward)

        global_processed_count += current_batch_size

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(current_batch_size)
        curr_avg = sum(global_final_rewards) / len(global_final_rewards) if global_final_rewards else 0.0
        pbar.set_postfix({"Avg": f"{curr_avg:.3f}", "Processed": f"{global_processed_count}"})

    pbar.close()

    # =================================================

    print("\n" + "=" * 60)
    print("ğŸ“Š Final Evaluation Report")
    print("=" * 60)

    total_evaluated = len(global_final_rewards)
    overall_avg = sum(global_final_rewards) / total_evaluated if total_evaluated > 0 else 0.0
    finished_count = sum(1 for r in global_final_rewards if r > 0.0)

    print(f"Total Instances Processed: {total_evaluated}")
    print(f"Overall Average Reward   : {overall_avg:.4f}")
    print(f"Finished (reward > 0)    : {finished_count}/{total_evaluated}")

    if GROUP_N > 1:
        print("-" * 60)
        print(f"{'Group ID':<10} | {'Count':<10} | {'Avg Reward':<12} | {'Pass Rate':<12}")
        print("-" * 60)

        for g_id in range(GROUP_N):
            rewards = group_stats[g_id]
            count = len(rewards)
            if count > 0:
                avg_r = sum(rewards) / count
                pass_r = sum(1 for r in rewards if r >= 1.0) / count
            else:
                avg_r = 0.0
                pass_r = 0.0

            print(f"Sample {g_id:<3} | {count:<10} | {avg_r:.4f}       | {pass_r:.2%}")


if __name__ == "__main__":
    main()
