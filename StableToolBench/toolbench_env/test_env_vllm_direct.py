"""
ä¸ test_single_chain_env_tokenized.py é€»è¾‘ä¸€è‡´ï¼Œä½†å°†ã€Œè°ƒç”¨ serve APIã€æ”¹ä¸ºã€ŒvLLM è¿›ç¨‹å†…ç›´æ¥ generateã€ã€‚
å‚æ•°ä¸ç”Ÿæˆé€»è¾‘ä¸ vllm_rollout_spmd ä¿æŒä¸€è‡´ï¼Œèƒ½å¤ç”¨çš„ç›´æ¥å¤ç”¨ã€‚
"""

# å¿…é¡»åœ¨ import vllm ä¹‹å‰è®¾ç½®ï¼Œå¦åˆ™ä¼šè§¦å‘ "Cannot re-initialize CUDA in forked subprocess"
import os

os.environ.setdefault("VLLM_WORKER_MULTIPROC_METHOD", "spawn")

import json
import time
from dataclasses import dataclass
from typing import List, Optional

# ==========================================
from build_env import build_toolbench_envs
from omegaconf import OmegaConf
from tqdm import tqdm
from transformers import AutoTokenizer

# ä¸ vllm_rollout_spmd ä¸€è‡´ï¼šå¤ç”¨ verl å·¥å…·
from verl.utils.torch_functional import pad_2d_list_to_length

# ================= é…ç½®åŒºåŸŸ =================
ENV_NUM = 100
GROUP_N = 1
SEED = 42
MAX_SAMPLES = 200

# æ˜¯å¦æ”¶é›†æ¯ä¸ª episode çš„ observation è½¨è¿¹å¹¶å†™å…¥ç»“æœ JSON
COLLECT_OBSERVATIONS = True
# å†™å…¥ JSON æ—¶æ¯ä¸ª observation å­—ç¬¦ä¸²çš„æœ€å¤§é•¿åº¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼›None è¡¨ç¤ºä¸æˆªæ–­
MAX_OBS_LENGTH_SAVED = 2048

os.environ["NO_PROXY"] = "localhost,127.0.0.1"
# MODEL_PATH = "/home/u-longyy/efficient-verl-agent/checkpoints/verl_agent_toolbench_eval/grpo_qwen2.5_7b_toolbench/global_step_50/actor"
MODEL_PATH = "Qwen/Qwen2.5-7B-Instruct"

TRUNCATE_PROMPT_TOKENS = 7072

# ä¸ vllm_rollout_spmd ä½¿ç”¨çš„ config ç»“æ„ä¸€è‡´ï¼ˆeval.sh + ppo_trainer.yamlï¼‰
# ç”¨äºä¸ vllMRollout ç›¸åŒçš„ kwargs æ„å»ºå’Œ pad_2d_list_to_length
ROLLOUT_CONFIG = OmegaConf.create(
    {
        "prompt_length": 7072,
        "response_length": 1024,
        "temperature": 1.0,
        "top_k": -1,
        "top_p": 1,
        "n": 1,
        "logprobs": 0,
        "do_sample": True,
        "enforce_eager": True,
        "dtype": "bfloat16",
        "gpu_memory_utilization": 0.85,
        "max_num_batched_tokens": 15000,
        "max_num_seqs": 200,
        "load_format": "safetensors",
        "disable_log_stats": True,
        "enable_chunked_prefill": False,
        "enable_prefix_caching": True,
        "seed": 0,
        "val_kwargs": {
            "temperature": 1.0,
            "top_p": 1.0,
            "top_k": -1,
            "n": 1,
        },
    }
)


@dataclass
class SpecificArgs:
    input_query_dir: str = "data/test_instruction/G1_instruction.json"
    corpus_tsv_path: str = None
    retrieval_model_path: str = None
    max_sequence_length: int = 1024
    evaluator_name: str = "tooleval_deepseek-normalization"
    evaluators_cfg_path: str = "toolbench/tooleval/evaluators"
    template: str = "chat_model"
    single_chain_max_step: int = 10
    evaluation_times: int = 1
    model_path: str = MODEL_PATH
    tool_root_dir: str = "data/toolenv/tools/"
    toolbench_key: str = "EMPTY"
    rapidapi_key: str = "EMPTY"
    use_rapidapi_key: bool = False
    api_customization: bool = False
    use_retriever: bool = False
    max_observation_length: int = 1024
    observ_compress_method: str = "truncate"
    base_url: str = "http://127.0.0.1:8000/v1"  # ä»…ç”¨äº env å†…éƒ¨å¯é€‰é€»è¾‘ï¼Œæœ¬è„šæœ¬ä¸è°ƒ serve
    method: str = "DFS"
    tree_beam_size: int = 2
    max_query_count: int = 200
    answer: int = 1


# ===========================================


def get_vllm_llm_and_tokenizer():
    """è¿›ç¨‹å†…åŠ è½½ vLLM ä¸ tokenizerï¼Œå‚æ•°ä¸ vllm_rollout_spmd ä¸€è‡´"""
    try:
        from vllm import LLM
    except ImportError as e:
        print(f"âŒ æœªå®‰è£… vllm: {e}")
        return None, None

    config = ROLLOUT_CONFIG
    max_model_len = config.prompt_length + config.response_length

    try:
        print(f"â³ Loading vLLM model: {MODEL_PATH}...")
        load_format = "dummy" if str(config.load_format).startswith("dummy") else config.load_format
        llm = LLM(
            model=MODEL_PATH,
            trust_remote_code=True,
            max_model_len=max_model_len,
            gpu_memory_utilization=config.gpu_memory_utilization,
            dtype=config.dtype,
            enforce_eager=config.enforce_eager,
            load_format=load_format,
            max_num_batched_tokens=config.max_num_batched_tokens,
            max_num_seqs=config.max_num_seqs,
            enable_chunked_prefill=config.enable_chunked_prefill,
            enable_prefix_caching=config.enable_prefix_caching,
            disable_log_stats=config.disable_log_stats,
            seed=config.get("seed", 0),
        )
        tokenizer = llm.get_tokenizer()
        print("âœ… vLLM and tokenizer loaded.")
        return llm, tokenizer
    except Exception as e:
        print(f"âŒ vLLM/Tokenizer load failed: {e}")
        return None, None


def get_hf_tokenizer():
    """ä¸ tokenized è„šæœ¬ä¸€è‡´ï¼šç”¨ HF AutoTokenizerï¼Œä¿è¯ prompt encode ä¸ client ç«¯å®Œå…¨ç›¸åŒ"""
    try:
        print(f"â³ Loading HF Tokenizer: {MODEL_PATH}...")
        tok = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("âœ… HF Tokenizer loaded.")
        return tok
    except Exception as e:
        print(f"âŒ HF Tokenizer load failed: {e}")
        return None


def generate_actions_vllm_direct(
    llm,
    tokenizer,
    hf_tokenizer,
    observations: List[str],
    dones: List[bool],
    args: SpecificArgs,
) -> List[str]:
    """vLLM è¿›ç¨‹å†… generateï¼ŒSamplingParams ä¸ response å¤„ç†ä¸ vllm_rollout_spmd ä¸€è‡´ã€‚"""
    from vllm import SamplingParams

    total = len(observations)
    actions = [None] * total
    active_indices = []
    active_prompts = []

    for i, (obs, done) in enumerate(zip(observations, dones)):
        if done:
            actions[i] = "FINISHED"
        elif not obs:
            actions[i] = ""
        else:
            active_indices.append(i)
            active_prompts.append(obs)

    if not active_prompts:
        return actions

    config = ROLLOUT_CONFIG
    # ä¸ tokenized å¯¹é½ï¼šç”¨ HF tokenizer çš„ eos/pad ä¸ decodeï¼Œä¾¿äºå’Œ client ç«¯è¡Œä¸ºä¸€è‡´
    enc_dec_tok = hf_tokenizer if hf_tokenizer is not None else tokenizer
    pad_token_id = enc_dec_tok.pad_token_id
    print(f"pad_token_id: {pad_token_id}")
    eos_token_id = enc_dec_tok.eos_token_id
    print(f"eos_token_id: {eos_token_id}")
    pad_id = pad_token_id if pad_token_id is not None else eos_token_id

    try:
        # prompt_token_idsï¼šä¸ tokenized å®Œå…¨ä¸€è‡´ï¼Œç”¨ HF tokenizer encodeï¼ˆclient ç«¯å‘æ¥çš„å°±æ˜¯ HF ç¼–ç ï¼‰
        prompt_token_ids_list = []
        for p in active_prompts:
            ids = (hf_tokenizer if hf_tokenizer is not None else tokenizer).encode(p, add_special_tokens=True)
            if len(ids) > TRUNCATE_PROMPT_TOKENS:
                print(f"Truncating prompt from {len(ids)} to {TRUNCATE_PROMPT_TOKENS}")
                ids = ids[-TRUNCATE_PROMPT_TOKENS:]
            prompt_token_ids_list.append(ids)

        vllm_inputs = [{"prompt_token_ids": ids} for ids in prompt_token_ids_list]
        for input_data in vllm_inputs:
            if not isinstance(input_data["prompt_token_ids"], list):
                input_data["prompt_token_ids"] = list(input_data["prompt_token_ids"])

        # ä¸ vllm_rollout_spmd ä¸€è‡´ï¼škwargs æ„å»º + val_kwargs è¦†ç›–ï¼›ä¸ä¼  seed ä»¥ä¸ client API è¡Œä¸ºä¸€è‡´ï¼ˆéç¡®å®šæ€§é‡‡æ ·ï¼‰
        kwargs = dict(
            n=1,
            logprobs=0,
            max_tokens=config.response_length,
        )
        kwargs["detokenize"] = False
        for k in config.keys():
            if k in ("val_kwargs", "seed"):
                continue
            if hasattr(SamplingParams(), str(k)):
                kwargs[k] = config.get(k)
        kwargs["top_k"] = config.val_kwargs.top_k
        kwargs["top_p"] = config.val_kwargs.top_p
        kwargs["temperature"] = config.val_kwargs.temperature
        sampling_params = SamplingParams(**kwargs)

        outputs = llm.generate(
            prompts=vllm_inputs,
            sampling_params=sampling_params,
            use_tqdm=False,
        )

        # ä¸ vllm_rollout_spmd ä¸€è‡´ï¼šresponse æ”¶é›†æ–¹å¼
        response = []
        for output in outputs:
            for sample_id in range(len(output.outputs)):
                response_ids = output.outputs[sample_id].token_ids
                response.append(response_ids)

        # ä¸ vllm_rollout_spmd ä¸€è‡´ï¼špad_2d_list_to_length
        response_padded = pad_2d_list_to_length(response, pad_id, max_length=config.response_length)

        # ä» padded å–æ¯è¡Œï¼Œæˆªæ–­åˆ°é¦–ä¸ª EOS/PAD å†è§£ç ï¼ˆä¸ tokenized ä¸€è‡´ç”¨åŒä¸€ tokenizer decodeï¼‰
        for i in range(len(active_indices)):
            original_idx = active_indices[i]
            row = response_padded[i].tolist()
            content_ids = []
            for tid in row:
                if tid == eos_token_id or tid == pad_id:
                    break
                content_ids.append(tid)
            text = enc_dec_tok.decode(content_ids, skip_special_tokens=True)
            actions[original_idx] = text.strip() if text else ""

    except Exception as e:
        print(f"âŒ vLLM generate failed: {e}")
        for idx in active_indices:
            if actions[idx] is None:
                actions[idx] = ""

    for i in range(total):
        if actions[i] is None:
            actions[i] = ""

    return actions


def _truncate_obs_for_save(obs: str, max_len: Optional[int]) -> str:
    """å†™å…¥ç»“æœæ—¶å¯¹å•æ¡ observation åšé•¿åº¦æˆªæ–­ã€‚"""
    if obs is None or max_len is None or max_len <= 0:
        return obs if obs is not None else ""
    if len(obs) <= max_len:
        return obs
    return f"[truncated, total {len(obs)} chars] ..." + obs[-max_len:]


def main():
    args = SpecificArgs()

    if not os.path.exists(args.input_query_dir):
        print(f"âŒ Input file not found: {args.input_query_dir}")
        return

    llm, tokenizer = get_vllm_llm_and_tokenizer()
    if llm is None or tokenizer is None:
        print("âŒ å¿…é¡»æˆåŠŸåŠ è½½ vLLM ä¸ Tokenizerï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    hf_tokenizer = get_hf_tokenizer()
    if hf_tokenizer is None:
        print("âš ï¸ HF Tokenizer æœªåŠ è½½ï¼Œå°†ç”¨ vLLM tokenizer åš prompt/decodeï¼Œä¸ client ç«¯å¯èƒ½ä¸ä¸€è‡´")

    try:
        envs = build_toolbench_envs(
            env_num=ENV_NUM,
            group_n=GROUP_N,
            resources_per_worker={"num_cpus": 100},
            is_train=False,
            specific_args=args,
        )
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæ„å»ºå¤±è´¥: {e}")
        return

    total_dataset_len = len(envs._query_list)

    print("ğŸš€ åˆå§‹åŒ– ToolBench å…¨é‡è¯„æµ‹ (vLLM è¿›ç¨‹å†… generate + æœ¬åœ° Tokenizeï¼Œä¸ serve tokenized å¯¹æ¯”)")
    print(f"Dataset Size: {total_dataset_len} | Batch Size: {ENV_NUM} | Group N: {GROUP_N}")

    group_stats = [[] for _ in range(GROUP_N)]
    global_processed_count = 0
    global_final_rewards = []
    # æ”¶é›†æ¯ä¸ª episode çš„ observation è½¨è¿¹ï¼ˆä»…å½“ COLLECT_OBSERVATIONS ä¸º True æ—¶å¡«å……ï¼‰
    global_episode_observations = []

    pbar_total = min(total_dataset_len, MAX_SAMPLES) if MAX_SAMPLES is not None else total_dataset_len
    pbar = tqdm(total=pbar_total, desc="Total Progress", unit="sample")

    while True:
        if global_processed_count >= MAX_SAMPLES:
            print(f"âœ… å·²è¾¾åˆ°æŒ‡å®šæ ·æœ¬æ•°é‡ {MAX_SAMPLES}ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break
        # import pdb; pdb.set_trace()
        observations, infos = envs.reset()

        current_batch_size = len(observations)
        batch_group_ids = []
        for i in range(current_batch_size):
            abs_index = global_processed_count + i
            group_id = abs_index % GROUP_N
            batch_group_ids.append(group_id)

        global_processed_count += current_batch_size
        dones = [False] * current_batch_size
        episode_rewards = [0.0] * current_batch_size
        final_rewards = [None] * current_batch_size
        # æœ¬ batch å†…æ¯ä¸ª env çš„ observation è½¨è¿¹ï¼ˆä»…å½“ COLLECT_OBSERVATIONS æ—¶ä½¿ç”¨ï¼‰
        episode_obs_trajectory = [[] for _ in range(current_batch_size)]
        # æœ¬ batch çš„è½¨è¿¹æŒ‰ env ç´¢å¼•æš‚å­˜ï¼Œæœ€åæŒ‰åº extendï¼Œä¿è¯ä¸ global_final_rewards ç´¢å¼•ä¸€è‡´
        batch_episode_observations = [None] * current_batch_size if COLLECT_OBSERVATIONS else None

        if COLLECT_OBSERVATIONS:
            for i in range(current_batch_size):
                episode_obs_trajectory[i].append(observations[i])

        step_cnt = 0
        max_steps = 15  # eval.sh: env.max_steps=15

        while not all(dones) and step_cnt < max_steps:
            step_cnt += 1
            actions = generate_actions_vllm_direct(llm, tokenizer, hf_tokenizer, observations, dones, args)
            next_obs, step_rewards, next_dones, step_infos = envs.step(actions)

            for i in range(current_batch_size):
                if not dones[i]:
                    episode_rewards[i] += step_rewards[i]
                    if COLLECT_OBSERVATIONS:
                        episode_obs_trajectory[i].append(next_obs[i])
                    if next_dones[i]:
                        final_rewards[i] = episode_rewards[i]
                        dones[i] = True
                        if COLLECT_OBSERVATIONS:
                            trajectory = [_truncate_obs_for_save(o, MAX_OBS_LENGTH_SAVED) for o in episode_obs_trajectory[i]]
                            batch_episode_observations[i] = trajectory
                    observations[i] = next_obs[i]

        # æœªåœ¨å¾ªç¯å†…ç»“æŸçš„ episodeï¼ˆå¦‚è¾¾åˆ° max_stepsï¼‰ä¹Ÿå†™å…¥æœ¬ batch çš„å¯¹åº”æ§½ä½
        if COLLECT_OBSERVATIONS:
            for i in range(current_batch_size):
                if batch_episode_observations[i] is not None:
                    continue
                if not episode_obs_trajectory[i]:
                    continue
                trajectory = [_truncate_obs_for_save(o, MAX_OBS_LENGTH_SAVED) for o in episode_obs_trajectory[i]]
                batch_episode_observations[i] = trajectory
            global_episode_observations.extend(batch_episode_observations)

        import pdb

        pdb.set_trace()

        for i in range(current_batch_size):
            reward = final_rewards[i] if final_rewards[i] is not None else 0.0
            global_final_rewards.append(reward)
            g_id = batch_group_ids[i]
            group_stats[g_id].append(reward)

        pbar.update(current_batch_size)
        curr_avg = sum(global_final_rewards) / len(global_final_rewards) if global_final_rewards else 0
        pbar.set_postfix({"Avg": f"{curr_avg:.3f}"})

    pbar.close()

    # ================= æœ€ç»ˆæŠ¥å‘Š =================
    print("\n" + "=" * 60)
    print("ğŸ“Š Final Evaluation Report (vLLM Direct Generate + Tokenized)")
    print("=" * 60)

    total_evaluated = len(global_final_rewards)
    overall_avg = sum(global_final_rewards) / total_evaluated if total_evaluated > 0 else 0.0

    print(f"Total Instances Processed: {total_evaluated}")
    print(f"Overall Average Reward   : {overall_avg:.4f}")
    print("-" * 60)
    print(f"{'Group ID':<10} | {'Count':<10} | {'Avg Reward':<12} | {'Pass Rate':<12}")
    print("-" * 60)

    group_results_json = {}
    for g_id in range(GROUP_N):
        rewards = group_stats[g_id]
        count = len(rewards)
        if count > 0:
            avg_r = sum(rewards) / count
            pass_r = sum(1 for r in rewards if r >= 1.0) / count
        else:
            avg_r = 0.0
            pass_r = 0.0
        print(f"Sample {g_id:<3} | {count:<10} | {avg_r:.4f}       | {pass_r:.2%}")
        group_results_json[f"sample_{g_id}"] = {
            "count": count,
            "average_reward": avg_r,
            "pass_rate": pass_r,
            "rewards": rewards,
        }

    result_file = f"eval_result_grouped_vllm_direct_{int(time.time())}.json"
    result_data = {
        "total_evaluated": total_evaluated,
        "overall_average": overall_avg,
        "group_n": GROUP_N,
        "group_details": group_results_json,
        "all_rewards_flat": global_final_rewards,
        "backend": "vllm_direct_generate_tokenized",
    }
    if COLLECT_OBSERVATIONS and global_episode_observations:
        result_data["episode_observations"] = global_episode_observations
        result_data["max_obs_length_saved"] = MAX_OBS_LENGTH_SAVED
    with open(result_file, "w") as f:
        json.dump(result_data, f, indent=4)
    print(f"\nğŸ“ è¯¦ç»†åˆ†ç»„ç»“æœå·²ä¿å­˜è‡³: {result_file}")
    if COLLECT_OBSERVATIONS and global_episode_observations:
        print(f"   (å·²æ”¶é›† {len(global_episode_observations)} æ¡ episode çš„ observation è½¨è¿¹)")


if __name__ == "__main__":
    # é¿å… "Cannot re-initialize CUDA in forked subprocess"ï¼šå¼ºåˆ¶ä½¿ç”¨ spawnï¼Œ
    # ä½¿åç»­ vLLM/å¤šè¿›ç¨‹ä½¿ç”¨ spawn è€Œé forkï¼Œä¸ CUDA å…¼å®¹ã€‚
    import multiprocessing

    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        pass  # å·²è®¾ç½®è¿‡åˆ™å¿½ç•¥
    main()
