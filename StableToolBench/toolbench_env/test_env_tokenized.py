import json
import os
import time
from dataclasses import dataclass
from typing import List

# ==========================================
from build_env import build_toolbench_envs
from openai import OpenAI
from tqdm import tqdm
from transformers import AutoTokenizer

# ==========================================

# ================= é…ç½®åŒºåŸŸ =================
ENV_NUM = 163
GROUP_N = 1  # æ¯”å¦‚è¿™é‡Œè®¾ä¸º3ï¼Œè¡¨ç¤ºæ¯ä¸ªQueryé‡‡æ ·3æ¬¡
SEED = 42
MAX_SAMPLES = 163  # æŒ‡å®šæµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNone è¡¨ç¤ºè·‘å®Œå…¨éƒ¨æ•°æ®ï¼›è®¾ä¸ºæ•´æ•°åˆ™è¾¾åˆ°è¯¥æ•°é‡ååœæ­¢

os.environ["NO_PROXY"] = "localhost,127.0.0.1"
VLLM_API_URL = "http://127.0.0.1:8000/v1"
#VLLM_MODEL_NAME = "Qwen/Qwen3-8B"
#MODEL_PATH = "Qwen/Qwen3-8B"
#VLLM_MODEL_NAME = "ToolBench/ToolLLaMA-2-7b-v2"

VLLM_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
MODEL_PATH = "Qwen/Qwen3-4B-Instruct-2507"

STOP_TOKENS = ["<|im_end|>", "<|endoftext|>", "</s>"]


@dataclass
class SpecificArgs:
    input_query_dir: str = "solvable_queries/test_instruction/G1_instruction.json"
    corpus_tsv_path: str = None
    retrieval_model_path: str = None
    max_sequence_length: int = 1024
    evaluator_name: str = "tooleval_aliyun-deepseek-normalization"
    evaluators_cfg_path: str = "toolbench/tooleval/evaluators"
    #template: str = "chat_model"
    template: str = "tool-llama-single-round"
    single_chain_max_step: int = 12
    evaluation_times: int = 1
    model_path: str = MODEL_PATH
    tool_root_dir: str = "tools_folder/server_cache/tools"
    toolbench_key: str = "EMPTY"
    rapidapi_key: str = "EMPTY"
    use_rapidapi_key: bool = False
    api_customization: bool = False
    use_retriever: bool = False
    max_observation_length: int = 1024
    observ_compress_method: str = "truncate"
    base_url: str = VLLM_API_URL
    method: str = "CoT"
    tree_beam_size: int = 4
    max_query_count: int = 200
    answer: int = 1


# ===========================================


def get_vllm_client():
    return OpenAI(base_url=VLLM_API_URL, api_key="EMPTY")


def get_tokenizer():
    try:
        print(f"â³ Loading Tokenizer: {MODEL_PATH}...")
        # ç¡®ä¿åŠ è½½ tokenizerï¼Œè¿™å¯¹äºæœ¬åœ° tokenization æ˜¯å¿…é¡»çš„
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("âœ… Tokenizer loaded.")
        return tokenizer
    except Exception as e:
        print(f"âŒ Tokenizer load failed: {e}")
        return None


def generate_actions_openai_batch(
    client: OpenAI,
    tokenizer: AutoTokenizer,
    observations: List[str],
    dones: List[bool],
    args: SpecificArgs,
) -> List[str]:
    """ç”ŸæˆåŠ¨ä½œ Batch"""
    total = len(observations)
    actions = [None] * total
    active_indices = []
    active_prompts = []

    # ç­›é€‰å‡ºéœ€è¦ç”Ÿæˆçš„ prompt
    for i, (obs, done) in enumerate(zip(observations, dones)):
        if done:
            actions[i] = "FINISHED"
        elif not obs:
            actions[i] = ""
        else:
            active_indices.append(i)
            active_prompts.append(obs)

    if not active_prompts:
        return actions

    try:
        prompt_inputs = active_prompts

        # ============ ä¿®æ”¹å¼€å§‹: Tokenize ============
        if tokenizer:
            # ä½¿ç”¨ tokenizer.encode å°†æ–‡æœ¬è½¬æ¢ä¸º token id åˆ—è¡¨
            # ç»“æœå½¢å¼ä¸º [[id1, id2, ...], [id1, id2, ...]]
            # æ³¨æ„ï¼švLLM/OpenAI æ¥å£æ”¯æŒ prompt å‚æ•°ä¸º List[List[int]]
            prompt_inputs = [tokenizer.encode(p, add_special_tokens=True) for p in active_prompts]
        else:
            print("âš ï¸ Warning: Tokenizer is missing, falling back to raw text string.")
        # ============ ä¿®æ”¹ç»“æŸ ============
        for i, prompt_input in enumerate(prompt_inputs):
            if len(prompt_input) > 2 * 4096 - args.max_sequence_length:
                prompt_inputs[i] = prompt_input[-(2 * 4096 - args.max_sequence_length) :]

        """
        response = client.completions.create(
            model=VLLM_MODEL_NAME,
            prompt=prompt_inputs,  # è¿™é‡Œä¼ å…¥çš„æ˜¯ token ids åˆ—è¡¨ï¼ˆå¦‚æœ tokenizer å­˜åœ¨ï¼‰
            max_tokens=args.max_sequence_length,
            temperature=1.0,
            # seed=SEED,  # ä¸ test_vllm_direct ç«¯ç›¸åŒ seedï¼Œä¿è¯é‡‡æ ·ä¸€è‡´ã€æ¶ˆé™¤ reward å·®è·
            # stop=STOP_TOKENS,
            extra_body={
                "truncate_prompt_tokens": 4096,  # å¦‚æœ token ä»ç„¶è¿‡é•¿ï¼ŒvLLM ä¼šæ ¹æ®æ­¤å‚æ•°æˆªæ–­
            },
        )
        """
        extra_body = {
            "truncate_prompt_tokens": 2 * 4096 - args.max_sequence_length,
            # "top_p": 0.8,
            # "top_k": 20,
            # "repetition_penalty": 1.05,
            # "frequency_penalty": 0.0,
        }
        if getattr(tokenizer, "eos_token_id", None) is not None:
            extra_body["stop_token_ids"] = [tokenizer.eos_token_id]
        response = client.completions.create(
            model=VLLM_MODEL_NAME,
            prompt=prompt_inputs,
            max_tokens=args.max_sequence_length,
            temperature=1.0,
            n=1,
            logprobs=0,
            extra_body=extra_body,
        )

        # é€ä¸ªæ‰“å°å®é™…prompté•¿åº¦
        print(f"å®é™…prompté•¿åº¦: {response.usage.prompt_tokens}")

        choices = response.choices
        for i, choice in enumerate(choices):
            if i < len(active_indices):
                original_idx = active_indices[i]
                actions[original_idx] = choice.text.strip()

    except Exception as e:
        print(f"âŒ LLM Request Failed: {e}")
        for idx in active_indices:
            if actions[idx] is None:
                actions[idx] = ""

    for i in range(total):
        if actions[i] is None:
            actions[i] = ""

    return actions


def main():
    args = SpecificArgs()

    if not os.path.exists(args.input_query_dir):
        print(f"âŒ Input file not found: {args.input_query_dir}")
        return

    client = get_vllm_client()
    tokenizer = get_tokenizer()

    if tokenizer is None:
        print("âŒ å¿…é¡»æˆåŠŸåŠ è½½ Tokenizer æ‰èƒ½è¿›è¡Œæœ¬åœ° Tokenize æ“ä½œï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    try:
        envs = build_toolbench_envs(
            env_num=ENV_NUM,
            group_n=GROUP_N,
            resources_per_worker={"num_cpus": 100},
            is_train=False,
            specific_args=args,
        )
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæ„å»ºå¤±è´¥: {e}")
        return

    total_dataset_len = len(envs._query_list)

    print("ğŸš€ åˆå§‹åŒ– ToolBench å…¨é‡è¯„æµ‹ (With Client-Side Tokenization)")
    print(f"Dataset Size: {total_dataset_len} | Batch Size: {ENV_NUM} | Group N: {GROUP_N}")

    # ================= çŠ¶æ€è¿½è¸ªå˜é‡ =================
    group_stats = [[] for _ in range(GROUP_N)]
    global_processed_count = 0
    global_final_rewards = []

    pbar_total = min(total_dataset_len, MAX_SAMPLES) if MAX_SAMPLES is not None else total_dataset_len
    pbar = tqdm(total=pbar_total, desc="Total Progress", unit="sample")

    while True:
        if global_processed_count >= MAX_SAMPLES:
            print(f"âœ… å·²è¾¾åˆ°æŒ‡å®šæ ·æœ¬æ•°é‡ {MAX_SAMPLES}ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break
        observations, infos = envs.reset()

        current_batch_size = len(observations)

        # è®¡ç®—å½“å‰ Batch æ¯ä¸ªä»»åŠ¡çš„ Group ID
        batch_group_ids = []
        for i in range(current_batch_size):
            abs_index = global_processed_count + i
            group_id = abs_index % GROUP_N
            batch_group_ids.append(group_id)

        global_processed_count += current_batch_size

        dones = [False] * current_batch_size
        episode_rewards = [0.0] * current_batch_size
        final_rewards = [None] * current_batch_size

        step_cnt = 0
        max_steps = 15

        while not all(dones) and step_cnt < max_steps:
            step_cnt += 1
            actions = generate_actions_openai_batch(client, tokenizer, observations, dones, args)
            next_obs, step_rewards, next_dones, step_infos = envs.step(actions)
            # import pdb; pdb.set_trace()

            for i in range(current_batch_size):
                if not dones[i]:
                    episode_rewards[i] += step_rewards[i]
                    if next_dones[i]:
                        final_rewards[i] = episode_rewards[i]
                        dones[i] = True
                    observations[i] = next_obs[i]
            # print(f"Step:{step_cnt}") # å‡å°‘æ‰“å°åˆ·å±

        for i in range(current_batch_size):
            reward = final_rewards[i] if final_rewards[i] is not None else 0.0
            global_final_rewards.append(reward)

            g_id = batch_group_ids[i]
            group_stats[g_id].append(reward)

        pbar.update(current_batch_size)
        curr_avg = sum(global_final_rewards) / len(global_final_rewards) if global_final_rewards else 0
        pbar.set_postfix({"Avg": f"{curr_avg:.3f}"})

    pbar.close()

    # ================= æœ€ç»ˆæŠ¥å‘Š =================
    print("\n" + "=" * 60)
    print("ğŸ“Š Final Evaluation Report (Grouped)")
    print("=" * 60)

    total_evaluated = len(global_final_rewards)
    overall_avg = sum(global_final_rewards) / total_evaluated if total_evaluated > 0 else 0.0

    print(f"Total Instances Processed: {total_evaluated}")
    print(f"Overall Average Reward   : {overall_avg:.4f}")
    print("-" * 60)
    print(f"{'Group ID':<10} | {'Count':<10} | {'Avg Reward':<12} | {'Pass Rate':<12}")
    print("-" * 60)

    group_results_json = {}

    for g_id in range(GROUP_N):
        rewards = group_stats[g_id]
        count = len(rewards)
        if count > 0:
            avg_r = sum(rewards) / count
            pass_r = sum(1 for r in rewards if r >= 1.0) / count
        else:
            avg_r = 0.0
            pass_r = 0.0

        print(f"Sample {g_id:<3} | {count:<10} | {avg_r:.4f}       | {pass_r:.2%}")

        group_results_json[f"sample_{g_id}"] = {
            "count": count,
            "average_reward": avg_r,
            "pass_rate": pass_r,
            "rewards": rewards,
        }

    result_file = f"eval_result_grouped_{int(time.time())}.json"
    with open(result_file, "w") as f:
        json.dump(
            {
                "total_evaluated": total_evaluated,
                "overall_average": overall_avg,
                "group_n": GROUP_N,
                "group_details": group_results_json,
                "all_rewards_flat": global_final_rewards,
            },
            f,
            indent=4,
        )
    print(f"\nğŸ“ è¯¦ç»†åˆ†ç»„ç»“æœå·²ä¿å­˜è‡³: {result_file}")


if __name__ == "__main__":
    main()
